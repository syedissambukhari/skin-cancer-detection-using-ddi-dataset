{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cN2rsaCpcYNC",
        "outputId": "13cf5cea-ff60-4287-bf82-9564dbc14b78"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# needed to import dataset from google drive into colab\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd gdrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33b5qrPcnzj",
        "outputId": "38a61349-baf5-46e7-d3ac-6848f0918bcf"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd MyDrive"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q-OvvWRTc3s_",
        "outputId": "a67f69d5-a339-45c0-8a76-5a6f34e1a845"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install git+https://github.com/openai/CLIP.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Bnkuzurc_dU",
        "outputId": "20e02831-8791-4e2b-ef4d-a10d357a9900"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-jhwg4m6r\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-jhwg4m6r\n",
            "  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting ftfy (from clip==1.0)\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2022.10.31)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.0.1+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.15.2+cu118)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.6)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.7.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->clip==1.0) (16.0.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.22.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n",
            "Building wheels for collected packages: clip\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for clip: filename=clip-1.0-py3-none-any.whl size=1369498 sha256=5e6eaba48459a18f63d02c142a49cd7f6ea4faa941c73bf7140bc388e197a0a5\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-z_ika399/wheels/da/2b/4c/d6691fa9597aac8bb85d2ac13b112deb897d5b50f5ad9a37e4\n",
            "Successfully built clip\n",
            "Installing collected packages: ftfy, clip\n",
            "Successfully installed clip-1.0 ftfy-6.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import transforms\n",
        "import clip\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "JADCT01mc7RG"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn_data_folder = 'cnn_dataset'"
      ],
      "metadata": {
        "id": "bumYHpvJdJnP"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 7: Load and Preprocess Images for Nature Classification\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, data_folder, transform=None):\n",
        "        self.data_folder = data_folder\n",
        "        self.transform = transform\n",
        "        self.images, self.labels = self.load_images()\n",
        "\n",
        "    def load_images(self):\n",
        "        images = []\n",
        "        labels = []\n",
        "        for class_label, class_name in enumerate(os.listdir(self.data_folder)):\n",
        "            class_folder = os.path.join(self.data_folder, class_name)\n",
        "            if not os.path.isdir(class_folder):\n",
        "                continue\n",
        "            for file_name in os.listdir(class_folder):\n",
        "                file_path = os.path.join(class_folder, file_name)\n",
        "                image = cv2.imread(file_path)\n",
        "                image = cv2.resize(image, (224, 224))  # Resize image to fit CLIP model input shape\n",
        "                label = class_label\n",
        "                images.append(image)\n",
        "                labels.append(label)\n",
        "        return np.array(images), np.array(labels)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.images[index]\n",
        "        label = self.labels[index]\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "E5cZvhB9dR2M"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformation to normalize and convert image to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])"
      ],
      "metadata": {
        "id": "pVmpjOU_dR5A"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the fine-tuned CLIP model and tokenizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)"
      ],
      "metadata": {
        "id": "8xsIgUdvdR8Y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 8: Model Training for Nature Classification\n",
        "dataset = ImageDataset(cnn_data_folder, transform=transform)\n",
        "train_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "0JIN1I2odZ07"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the classification head\n",
        "num_classes = len(set(dataset.labels))\n",
        "classifier_head = nn.Linear(512, num_classes).to(device)"
      ],
      "metadata": {
        "id": "Pc4HDyDDdZ4o"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimizer and loss function\n",
        "optimizer = optim.Adam(classifier_head.parameters(), lr=0.001)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "OfJ2jBf_eY6R"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the classification head\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "    classifier_head.train()\n",
        "    total_loss = 0.0\n",
        "    total_correct = 0\n",
        "    total_samples = 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            image_features = model.encode_image(images)\n",
        "\n",
        "        # Convert the data type of image_features to match the classifier's weight matrix\n",
        "        image_features = image_features.to(classifier_head.weight.dtype)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        logits = classifier_head(image_features)\n",
        "\n",
        "        # Convert logits data type to torch.float32 for softmax\n",
        "        logits = logits.to(torch.float32)\n",
        "\n",
        "        loss = criterion(logits, labels)  # No need to convert labels to torch.long data type\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        _, predicted_labels = logits.max(1)\n",
        "        total_correct += (predicted_labels == labels).sum().item()\n",
        "        total_samples += labels.size(0)\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = total_correct / total_samples\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}] - Loss: {total_loss:.4f} - Accuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFci99mWddoD",
        "outputId": "ad9dbbf3-e1f8-462c-8e50-468482277f3d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/100] - Loss: 13.2779 - Accuracy: 0.9145\n",
            "Epoch [2/100] - Loss: 13.3903 - Accuracy: 0.9135\n",
            "Epoch [3/100] - Loss: 13.3258 - Accuracy: 0.9195\n",
            "Epoch [4/100] - Loss: 13.2284 - Accuracy: 0.9150\n",
            "Epoch [5/100] - Loss: 13.1805 - Accuracy: 0.9170\n",
            "Epoch [6/100] - Loss: 13.1766 - Accuracy: 0.9160\n",
            "Epoch [7/100] - Loss: 13.1125 - Accuracy: 0.9200\n",
            "Epoch [8/100] - Loss: 13.0618 - Accuracy: 0.9205\n",
            "Epoch [9/100] - Loss: 13.0584 - Accuracy: 0.9165\n",
            "Epoch [10/100] - Loss: 12.9619 - Accuracy: 0.9180\n",
            "Epoch [11/100] - Loss: 13.0173 - Accuracy: 0.9190\n",
            "Epoch [12/100] - Loss: 12.8627 - Accuracy: 0.9210\n",
            "Epoch [13/100] - Loss: 12.9591 - Accuracy: 0.9210\n",
            "Epoch [14/100] - Loss: 12.8907 - Accuracy: 0.9195\n",
            "Epoch [15/100] - Loss: 12.7997 - Accuracy: 0.9205\n",
            "Epoch [16/100] - Loss: 12.8200 - Accuracy: 0.9190\n",
            "Epoch [17/100] - Loss: 12.7412 - Accuracy: 0.9225\n",
            "Epoch [18/100] - Loss: 12.8058 - Accuracy: 0.9210\n",
            "Epoch [19/100] - Loss: 12.6636 - Accuracy: 0.9235\n",
            "Epoch [20/100] - Loss: 12.6906 - Accuracy: 0.9175\n",
            "Epoch [21/100] - Loss: 12.7250 - Accuracy: 0.9230\n",
            "Epoch [22/100] - Loss: 12.6466 - Accuracy: 0.9220\n",
            "Epoch [23/100] - Loss: 12.6496 - Accuracy: 0.9245\n",
            "Epoch [24/100] - Loss: 12.6060 - Accuracy: 0.9260\n",
            "Epoch [25/100] - Loss: 12.5449 - Accuracy: 0.9230\n",
            "Epoch [26/100] - Loss: 12.5429 - Accuracy: 0.9235\n",
            "Epoch [27/100] - Loss: 12.4676 - Accuracy: 0.9220\n",
            "Epoch [28/100] - Loss: 12.5532 - Accuracy: 0.9255\n",
            "Epoch [29/100] - Loss: 12.3749 - Accuracy: 0.9250\n",
            "Epoch [30/100] - Loss: 12.5700 - Accuracy: 0.9230\n",
            "Epoch [31/100] - Loss: 12.4059 - Accuracy: 0.9225\n",
            "Epoch [32/100] - Loss: 12.3424 - Accuracy: 0.9255\n",
            "Epoch [33/100] - Loss: 12.3305 - Accuracy: 0.9235\n",
            "Epoch [34/100] - Loss: 12.3153 - Accuracy: 0.9250\n",
            "Epoch [35/100] - Loss: 12.4665 - Accuracy: 0.9230\n",
            "Epoch [36/100] - Loss: 12.3927 - Accuracy: 0.9225\n",
            "Epoch [37/100] - Loss: 12.3126 - Accuracy: 0.9235\n",
            "Epoch [38/100] - Loss: 12.3361 - Accuracy: 0.9255\n",
            "Epoch [39/100] - Loss: 12.2028 - Accuracy: 0.9245\n",
            "Epoch [40/100] - Loss: 12.1408 - Accuracy: 0.9240\n",
            "Epoch [41/100] - Loss: 12.1922 - Accuracy: 0.9245\n",
            "Epoch [42/100] - Loss: 12.1942 - Accuracy: 0.9260\n",
            "Epoch [43/100] - Loss: 12.2018 - Accuracy: 0.9275\n",
            "Epoch [44/100] - Loss: 12.0521 - Accuracy: 0.9255\n",
            "Epoch [45/100] - Loss: 12.0811 - Accuracy: 0.9270\n",
            "Epoch [46/100] - Loss: 12.0562 - Accuracy: 0.9260\n",
            "Epoch [47/100] - Loss: 12.1652 - Accuracy: 0.9240\n",
            "Epoch [48/100] - Loss: 12.1125 - Accuracy: 0.9255\n",
            "Epoch [49/100] - Loss: 11.9734 - Accuracy: 0.9255\n",
            "Epoch [50/100] - Loss: 11.9917 - Accuracy: 0.9275\n",
            "Epoch [51/100] - Loss: 12.0272 - Accuracy: 0.9275\n",
            "Epoch [52/100] - Loss: 11.9463 - Accuracy: 0.9250\n",
            "Epoch [53/100] - Loss: 11.9515 - Accuracy: 0.9235\n",
            "Epoch [54/100] - Loss: 11.9142 - Accuracy: 0.9270\n",
            "Epoch [55/100] - Loss: 12.0046 - Accuracy: 0.9275\n",
            "Epoch [56/100] - Loss: 11.8463 - Accuracy: 0.9285\n",
            "Epoch [57/100] - Loss: 11.7801 - Accuracy: 0.9300\n",
            "Epoch [58/100] - Loss: 11.8321 - Accuracy: 0.9270\n",
            "Epoch [59/100] - Loss: 11.7984 - Accuracy: 0.9265\n",
            "Epoch [60/100] - Loss: 11.7222 - Accuracy: 0.9310\n",
            "Epoch [61/100] - Loss: 11.6947 - Accuracy: 0.9300\n",
            "Epoch [62/100] - Loss: 11.8444 - Accuracy: 0.9305\n",
            "Epoch [63/100] - Loss: 11.6108 - Accuracy: 0.9305\n",
            "Epoch [64/100] - Loss: 11.7430 - Accuracy: 0.9290\n",
            "Epoch [65/100] - Loss: 11.7955 - Accuracy: 0.9265\n",
            "Epoch [66/100] - Loss: 11.6732 - Accuracy: 0.9285\n",
            "Epoch [67/100] - Loss: 11.7787 - Accuracy: 0.9260\n",
            "Epoch [68/100] - Loss: 11.7172 - Accuracy: 0.9265\n",
            "Epoch [69/100] - Loss: 11.7616 - Accuracy: 0.9305\n",
            "Epoch [70/100] - Loss: 11.9072 - Accuracy: 0.9305\n",
            "Epoch [71/100] - Loss: 11.6206 - Accuracy: 0.9315\n",
            "Epoch [72/100] - Loss: 11.6617 - Accuracy: 0.9320\n",
            "Epoch [73/100] - Loss: 11.4994 - Accuracy: 0.9315\n",
            "Epoch [74/100] - Loss: 11.6022 - Accuracy: 0.9295\n",
            "Epoch [75/100] - Loss: 11.6023 - Accuracy: 0.9290\n",
            "Epoch [76/100] - Loss: 11.5431 - Accuracy: 0.9310\n",
            "Epoch [77/100] - Loss: 11.5308 - Accuracy: 0.9275\n",
            "Epoch [78/100] - Loss: 11.4340 - Accuracy: 0.9295\n",
            "Epoch [79/100] - Loss: 11.5129 - Accuracy: 0.9290\n",
            "Epoch [80/100] - Loss: 11.4219 - Accuracy: 0.9320\n",
            "Epoch [81/100] - Loss: 11.4994 - Accuracy: 0.9295\n",
            "Epoch [82/100] - Loss: 11.5767 - Accuracy: 0.9290\n",
            "Epoch [83/100] - Loss: 11.4051 - Accuracy: 0.9325\n",
            "Epoch [84/100] - Loss: 11.3586 - Accuracy: 0.9295\n",
            "Epoch [85/100] - Loss: 11.4279 - Accuracy: 0.9285\n",
            "Epoch [86/100] - Loss: 11.4149 - Accuracy: 0.9320\n",
            "Epoch [87/100] - Loss: 11.3939 - Accuracy: 0.9330\n",
            "Epoch [88/100] - Loss: 11.3548 - Accuracy: 0.9310\n",
            "Epoch [89/100] - Loss: 11.3393 - Accuracy: 0.9320\n",
            "Epoch [90/100] - Loss: 11.3167 - Accuracy: 0.9315\n",
            "Epoch [91/100] - Loss: 11.2943 - Accuracy: 0.9335\n",
            "Epoch [92/100] - Loss: 11.2922 - Accuracy: 0.9330\n",
            "Epoch [93/100] - Loss: 11.4941 - Accuracy: 0.9280\n",
            "Epoch [94/100] - Loss: 11.2231 - Accuracy: 0.9320\n",
            "Epoch [95/100] - Loss: 11.2329 - Accuracy: 0.9305\n",
            "Epoch [96/100] - Loss: 11.3081 - Accuracy: 0.9310\n",
            "Epoch [97/100] - Loss: 11.1665 - Accuracy: 0.9300\n",
            "Epoch [98/100] - Loss: 11.2629 - Accuracy: 0.9320\n",
            "Epoch [99/100] - Loss: 11.2512 - Accuracy: 0.9340\n",
            "Epoch [100/100] - Loss: 11.2097 - Accuracy: 0.9320\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 9: Model Evaluation for Nature Classification\n",
        "nature_pred = []\n",
        "nature_labels = []\n",
        "classifier_head.eval()\n",
        "with torch.no_grad():\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        image_features = model.encode_image(images)\n",
        "\n",
        "        # Convert the data type of image_features to match the classifier's weight matrix\n",
        "        image_features = image_features.to(classifier_head.weight.dtype)\n",
        "\n",
        "        logits = classifier_head(image_features)\n",
        "        pred = logits.argmax(dim=-1).cpu().numpy()\n",
        "        nature_pred.extend(pred)\n",
        "        nature_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "nature_accuracy = accuracy_score(nature_labels, nature_pred)\n",
        "nature_precision = precision_score(nature_labels, nature_pred, average='weighted')\n",
        "nature_recall = recall_score(nature_labels, nature_pred, average='weighted')\n",
        "nature_f1_score = f1_score(nature_labels, nature_pred, average='weighted')\n",
        "\n",
        "print(\"Nature Accuracy:\", nature_accuracy)\n",
        "print(\"Nature Precision:\", nature_precision)\n",
        "print(\"Nature Recall:\", nature_recall)\n",
        "print(\"Nature F1-score:\", nature_f1_score)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJX7b_irddwf",
        "outputId": "63f6a4f7-9713-4ef9-a935-12677ad43320"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nature Accuracy: 0.9345\n",
            "Nature Precision: 0.9347717323327079\n",
            "Nature Recall: 0.9345\n",
            "Nature F1-score: 0.934489764025629\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 10: Save Nature Model\n",
        "# Save the fine-tuned classifier head\n",
        "nature_model_file = 'nature_model.pt'\n",
        "torch.save(classifier_head.state_dict(), nature_model_file)\n",
        "print(\"Nature Model saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDDxYPHFdoti",
        "outputId": "3c65cd7b-5ca7-45de-957f-1609b262f1a1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nature Model saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import clip\n",
        "import numpy as np\n",
        "from torchvision import transforms\n",
        "\n",
        "# Load the fine-tuned CLIP model and tokenizer\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model, preprocess = clip.load('ViT-B/32', device)\n",
        "\n",
        "# Define the classification head\n",
        "num_classes = 2  # Replace with the number of classes in your dataset\n",
        "classifier_head = nn.Linear(512, num_classes).to(device)\n",
        "classifier_head.load_state_dict(torch.load('nature_model.pt'))\n",
        "classifier_head.eval()\n",
        "\n",
        "# Define the mapping between class indices and class labels\n",
        "class_mapping = {\n",
        "    0: \"Islamophobic Image\",\n",
        "    1: \"Non Islamophobic Image\",\n",
        "}\n",
        "\n",
        "# Define the transformation to normalize and convert image to tensor\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "# Function to predict the class of an image\n",
        "def predict_class(image_path):\n",
        "    image = cv2.imread(image_path)\n",
        "    image = cv2.resize(image, (224, 224))  # Resize image to fit CLIP model input shape\n",
        "    image = transform(image).unsqueeze(0).to(device)  # Convert image to tensor and add batch dimension\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(image)\n",
        "\n",
        "    # Convert the data type of image_features to match the classifier's weight matrix\n",
        "    image_features = image_features.to(classifier_head.weight.dtype)\n",
        "\n",
        "    logits = classifier_head(image_features)\n",
        "    probabilities = nn.functional.softmax(logits, dim=1)\n",
        "    predicted_class_index = torch.argmax(probabilities, dim=1).item()\n",
        "    predicted_class_label = class_mapping[predicted_class_index]\n",
        "\n",
        "    return predicted_class_label\n",
        "\n",
        "# Test the prediction function\n",
        "image_path = 'cnn_dataset/Islamophobic image/islamophobic (1).jpg'\n",
        "predicted_class = predict_class(image_path)\n",
        "\n",
        "print(f\"Predicted Class: {predicted_class}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rREaTIKimK_4",
        "outputId": "74797c0e-5cf8-4f44-82e9-053d5cf892a8"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted Class: Islamophobic Image\n"
          ]
        }
      ]
    }
  ]
}